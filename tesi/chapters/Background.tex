\chapter{Background}
\label{chap:background}
Challenges brought by the new-generation of data-intensive applications require cooperation in the two fields of High Performance Computing and Big Data Analytics \cite{exa-big}. As a result, we have witnessed an evolution towards distributed computing, that allows to leverage a larger amount of storage and computing power by means of multiple interconnected machines  \cite{survey-distributed}. In the pursuing of the ``parallel revolution aggressive goal'' of ``writing parallel programs that scale with the number of cores and are as easy to write and efficient as sequential programs'' \cite{parallel-computing}, we are experiencing an increase in the amount of abstractions, tools, frameworks and libraries specifically targeted at helping the application developers writing parallel programs in an efficient and structured fashion. In this chapter we particularly focus on all those tools that help the programmer in structuring their programs without being required to handle the underlying communication infrastructure challenges.\newline

We consider the \textit{HPC communication layer} as \textit{``any framework, library, tool, protocol or pattern that eases the communication between distributed processes}'', and its only purpose is to abstract the user from the underlying infrastructure and protocols by providing a set of APIs to send and receive data. Following this line, and by considering the distinctions made by \cite{survey-distributed}, we can consider two main leading paradigms:
\begin{itemize}
	\item \textbf{Remote Invocation}: allows the user to call functions from one node to the other across the network. We consider in this category both \textit{Remote Method Invocation} (RMI) and \textit{Remote Procedure Call} (RPC), with the only distinction between the two being they are used to invoke object's methods or program functions, respectively. 
	\item \textbf{Message Oriented}: also in this category we can distinguish two paradigms, \textit{Message Passing Interface} (MPI) and \textit{Message Queuing} (MQ). The former being the de-facto standard in HPC, with minimal overhead, low level minimal abstraction and no fault tolerance; the latter, instead, offers queues to store intermediate messages still not received and it offers fault tolerance. 
\end{itemize}

By following this distinction, we list some of the most common communication framework and transports used in HPC and BDA environments, as also reported by \cite{survey-distributed}. However, we only give a brief description of each one of them, since the main aim of this chapter is to show the plethora of transports which can be found in the wild and which need to be considered when developing a communication related framework or library.\newline

\textbf{Java RMI} -  Java RMI system \cite{javarmi} is the de-facto standard in remote communication between Java Virtual Machines, since it provides an efficient mechanism for method invocations on Java objects residing in different machines. Its communication protocol is based on TCP/IP connections and is strictly integrated with the Java environment \cite{javaRMI-toHPC++}.\newline

\textbf{MPI} - The Message Passing Interface (MPI) \cite{mpi40} is a \textit{message-passing library interface specification}, it describes requirements targeting the message passing parallel programming model by providing a set of function specification that must be implemented to provide communication functionalities between processes. The main MPI's aim is standardization of message-passing functionalities to improve scalability, portability and ease-of-use. It is the de-facto standard for communications among processes in distributed systems, and various implementation of the specification exists, like OpenMPI \cite{openMPI} and MPICH \cite{MPICH}.\newline

\textbf{ZeroMQ} - ZeroMQ \cite{zeromq} is a communication library built on top of sockets which supports in-process, inter-process, TCP and multicast transports. It allows different communication patterns, like pub-sub, request-reply fan-out and offers an asynchronous I/O model for scalable multicore applications. It was the framework used in the first versions of FastFlow's distributed runtime.

\section{Towards Exascale computing}
%TODO aggiungere info da paper di ucx, libfabric e mercury dato che sembrano avere buone informazioni riguardo lo stato dell'arte e la necessit√† dei sistemi HPC in exascale computing
As illustrated in \cite{exa-big}, the path towards exascale computing requires leveraging the high-performance fabric networking interface offered by the various HPC systems. Thus, frameworks have emerged in order to completely leverage the functionality and performance requirements of customized High Performance Computing system's network hardware. We present here some of the frameworks which addressed this issue in recent years.\newline

\textbf{Unified Communication X} (UCX) - UCX \cite{ucx, ucx-website} is a network API framework for high throughput computing targeting modern interconnects with massive parallelism. It is designed to provide a set of interfaces for implementing multiple programming model libraries that are portable, scalable and efficient.\newline

\textbf{OpenFabrics Interface} (OFI) - OFI \cite{ofi, libfabric-website} is a collection of libraries, specifically designed to target scalability requirements of HPC systems, focused on exporting communication services to applications. The main aim of the set of libraries in OFI is to expose an interface to application programmers of the underlying network fabrics, in order to fulfill HPC users' needs.\newline

\textbf{Mercury} - Mercury \cite{mochi-core, mercury} is a framework implementing RPC and specifically designed for use in HPC environments. It offers an abstracted network API whose implementation is provided by a set of \textit{plug-in}s. It allows asynchronous execution of remote procedures and also offers Remote Memory Access (RMA) whenever the underlying transport fabric supports it.\newline

We have found Mercury's simple interface and broad support for various protocols particularly interesting, so we decided to integrate it as a communication framework for the distributed runtime of FastFlow. Moreover, Mercury's compatibility with multiple frameworks implementing high-performance HPC-related transports, allows maintaining portability and scalability over the heterogeneous environment of HPC network ecosystem. 